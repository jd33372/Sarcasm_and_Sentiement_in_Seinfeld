{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Seinfeld Dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Process\n",
    "\n",
    "I first filter out rows that contain the word \"Quote\" as they are not actual dialogue and for the purposes of this project I want to focus on the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   \n",
    "import pandas as pd\n",
    "\n",
    "seinfeld_data = pd.read_csv('C:/Users/james/OneDrive/Documents/Seths_Class_Repo/seinfeld_quotes.csv')\n",
    "\n",
    "seinfeld_data = seinfeld_data.drop_duplicates()\n",
    "\n",
    "word_to_remove = 'Quote'\n",
    "\n",
    "df_filtered = seinfeld_data[~seinfeld_data.apply(lambda row: row.astype(str).str.contains(word_to_remove).any(), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I Contruct the dialogue to character dataframe which where I will perform sentiment analysis, topic modeling, and sarcasm detection. I used regex to remove any text that was in brackets as most of it was not part of the dialogue. I then split the dialogue into character and quote columns on the colon that seperated the name from the dialogue. I then removed any rows that contained numbers and the last three rows that did not contain dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"Quote\"] = df_filtered[\"Quote\"].str.replace(r\"\\[.*?\\]\", \"\", regex=True).str.strip()\n",
    "\n",
    "def split_dialogue(text):\n",
    "    lines = text.split(\"\\n\")  \n",
    "    extracted = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()  \n",
    "        match = re.match(r\"([^:]+):\\s*(.*)\", line)  \n",
    "        if match:\n",
    "            extracted.append(match.groups())  \n",
    "    \n",
    "    return extracted\n",
    "\n",
    "full_clean = df_filtered['Quote'].apply(split_dialogue).explode().dropna().apply(pd.Series)\n",
    "\n",
    "full_clean.columns = ['Character', 'Quotes']\n",
    "\n",
    "full_clean = full_clean[~full_clean['Quotes'].str.match(r'^\\w+[^\\w\\s]$')]\n",
    "\n",
    "n = 3\n",
    "full_clean.drop(full_clean.tail(n).index, inplace = True)\n",
    "\n",
    "\n",
    "full_clean = full_clean[~full_clean['Quotes'].str.contains(r'\\d')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment analysis using vader. Applied using lambda function to the Quotes column. I then found the top 10 most frequently occuring characters and filtered the dataframe to only include those characters. I then grouped by character and found the average sentiment score for each character which I then plotted on horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "full_clean['Sentiment'] = full_clean['Quotes'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "top_characters = full_clean['Character'].value_counts().head(10).index\n",
    "\n",
    "top_character_sentiment = full_clean[full_clean['Character'].isin(top_characters)]\n",
    "\n",
    "\n",
    "avg_character_sentiment = top_character_sentiment.groupby('Character')['Sentiment'].mean().sort_values(ascending=False)\n",
    "\n",
    "avg_character_sentiment = pd.DataFrame(avg_character_sentiment)\n",
    "\n",
    "avg_character_sentiment.rename(columns={'Sentiment': 'Avg. Sentiment'}, inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "horizontal_bar_plot =sns.barplot(x='Avg. Sentiment', y='Character', data=avg_character_sentiment, palette='pastel')\n",
    "plt.title('Average Sentiment Score of Central Seinfeld Characters')\n",
    "plt.xlabel('Avg Sentiment')\n",
    "plt.ylabel('Character')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Topic Modeling\n",
    "\n",
    "Then I use BERTopic modeling to find the most prevalent topics in the show. I preprocessed the text by tokenizing and lemmatizing the text with a function along with removing stopwords. I then apply the model. I then visualize the topics using an intertopic distance map, a bar chart containing distribution of words within each topic, and a similarity matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "full_clean['Processed_Quotes'] = full_clean['Quotes'].apply(preprocess_text)\n",
    "\n",
    "model = BERTopic()\n",
    "topics, probabilities = model.fit_transform(full_clean['Processed_Quotes'])\n",
    "\n",
    "topics_info = model.get_topic_info()\n",
    "print(topics_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(top_n_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcasm Detection\n",
    "\n",
    "I implemented a pre trained hugging face model to detect sarcasm. I then applied the model to the text and assigned a label and score to each quote using a function. I then filtered the dataframe to only include the top 10 most frequently occuring characters and then found the percentage of time that they would say something sarcastic. I then plotted the results on a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"jkhan447/sarcasm-detection-Bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "sarcasm_detector = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sarcasm_detector.model.config.id2label\n",
    "\n",
    "def detect_sarcasm(text):\n",
    "    result = sarcasm_detector(text)[0]\n",
    "    return result['label'], result['score']\n",
    "\n",
    "full_clean[[\"Sarcasm_Label\", \"Sarcasm_Score\"]] = full_clean[\"Quotes\"].apply(lambda x: pd.Series(detect_sarcasm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_full_clean = full_clean[full_clean['Character'].isin(top_characters)]\n",
    "\n",
    "\n",
    "sarcastic_counts = top_full_clean[top_full_clean['Sarcasm_Label'] == 'LABEL_1']['Character'].value_counts()\n",
    "\n",
    "\n",
    "total_counts = top_full_clean['Character'].value_counts()\n",
    "\n",
    "\n",
    "sarcastic_percentage = (sarcastic_counts / total_counts) * 100\n",
    "\n",
    "sarcastic_percentage = sarcastic_percentage.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sarcastic_percentage.index, sarcastic_percentage.values, color='gold', edgecolor='black')\n",
    "plt.xlabel('Chance of Being Sarcastic (%)')\n",
    "plt.ylabel('Character')\n",
    "plt.title('Which Character is the Most Sarcastic?')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
